{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "import streamlit as st\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "\n",
        "# Configure Kafka connection details\n",
        "bootstrap_servers = 'rmp-8otv2.us-west8.gcp.confluent.cloud:9092'\n",
        "sasl_username = 'RAMPI7ZIW7OJEIAYU'\n",
        "sasl_password = 'MKEMAa6IumwzN40kXrxR9G6RPo9OBkakfO8rfSmSv5SrOFGeRnLNNnL9hEIZukkwe'\n",
        "kafka_topic = 'network-traffic'\n",
        "processed_topic = 'processed-data'\n",
        "\n",
        "# Define schema for the data\n",
        "schema = StructType([\n",
        "    StructField(\"source_ip\", StringType(), True),\n",
        "    StructField(\"destination_ip\", StringType(), True),\n",
        "    StructField(\"bytes_sent\", IntegerType(), True),\n",
        "    StructField(\"event_time\", TimestampType(), True)\n",
        "])\n",
        "\n",
        "# Start SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Real-Time Network Traffic Analysis\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Create a Streamlit app\n",
        "st.title(\"Real-Time Network Traffic Analysis\")\n",
        "fig, ax = plt.subplots()\n",
        "processed_data = []\n",
        "event_times = []\n",
        "\n",
        "@st.cache(allow_output_mutation=True)\n",
        "def get_spark_aggregated_df():\n",
        "    df = spark.readStream \\\n",
        "        .format(\"kafka\") \\\n",
        "        .option(\"kafka.bootstrap.servers\", bootstrap_servers) \\\n",
        "        .option(\"kafka.security.protocol\", \"SASL_SSL\") \\\n",
        "        .option(\"kafka.sasl.mechanism\", \"PLAIN\") \\\n",
        "        .option(\"kafka.sasl.username\", GKorir34) \\\n",
        "        .option(\"kafka.sasl.password\", yuteiy89_yu7) \\\n",
        "        .option(\"kafka.group.id\", \"network-traffic-group\") \\\n",
        "        .option(\"kafka.auto.offset.reset\", \"earliest\") \\\n",
        "        .option(\"subscribe\", kafka_topic) \\\n",
        "        .load()\n",
        "\n",
        "    # Process the data\n",
        "    processed_df = df.selectExpr(\"CAST(value AS STRING)\") \\\n",
        "        .select(from_json(\"value\", schema).alias(\"data\")) \\\n",
        "        .select(\"data.*\") \\\n",
        "        .groupBy(\"source_ip\") \\\n",
        "        .agg(sum(\"bytes_sent\").alias(\"total_bytes_sent\")) \\\n",
        "        .orderBy(desc(\"total_bytes_sent\"))\n",
        "\n",
        "    return processed_df\n",
        "\n",
        "@st.cache(allow_output_mutation=True)\n",
        "def get_streaming_query():\n",
        "    query = get_spark_aggregated_df().writeStream \\\n",
        "        .format(\"kafka\") \\\n",
        "        .option(\"kafka.bootstrap.servers\", bootstrap_servers) \\\n",
        "        .option(\"kafka.security.protocol\", \"SASL_SSL\") \\\n",
        "        .option(\"kafka.sasl.mechanism\", \"PLAIN\") \\\n",
        "        .option(\"kafka.sasl.username\", GKorir34) \\\n",
        "        .option(\"kafka.sasl.password\", yuteiy89_yu7) \\\n",
        "        .option(\"topic\", processed_topic) \\\n",
        "        .option(\"checkpointLocation\", \"/tmp/checkpoint\") \\\n",
        "        .outputMode(\"complete\") \\\n",
        "        .start()\n",
        "\n",
        "    return query\n",
        "\n",
        "def process_message(message):\n",
        "    nonlocal processed_data, event_times\n",
        "    if message is None:\n",
        "        return\n",
        "    if message.error():\n",
        "        if message.error().code() == KafkaError._PARTITION_EOF:\n",
        "            return\n",
        "        else:\n",
        "            st.error(f\"Error: {message.error()}\")\n",
        "            return\n",
        "\n",
        "    value = message.value().decode('utf-8')\n",
        "    data = value.split(',')\n",
        "    source_ip = data[0]\n",
        "    destination_ip = data[1]\n",
        "    bytes_sent = int(data[2])\n",
        "    event_time = datetime.now()\n",
        "\n",
        "    processed_data.append(bytes_sent)\n",
        "    event_times.append(event_time)\n",
        "    ax.clear()\n",
        "    ax.plot(event_times, processed_data)\n",
        "    ax.set_xlabel(\"Event Time\")\n",
        "    ax.set_ylabel(\"Processed Data\")\n",
        "    st.pyplot(fig)\n",
        "\n",
        "# Continuously read messages from Kafka topic\n",
        "def read_from_kafka():\n",
        "    consumer = get_spark_aggregated_df()\n",
        "    streaming_query = get_streaming_query()\n",
        "\n",
        "    while True:\n",
        "        message = consumer.poll(1.0)\n",
        "\n",
        "        if message is not None:\n",
        "            process_message(message)\n",
        "\n",
        "try:\n",
        "    read_from_kafka()\n",
        "except Exception as e:\n",
        "    st.error(f\"An error occurred: {str(e)}\")\n",
        "finally:\n",
        "    streaming_query.stop()\n",
        "    spark.stop()\n"
      ],
      "metadata": {
        "id": "w1X5HJEAe7P5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Kafka consumer configuration\n",
        "conf = {\n",
        "    'bootstrap.servers': bootstrap_servers,\n",
        "    'security.protocol': 'SASL_SSL',\n",
        "    'sasl.mechanisms': 'PLAIN',\n",
        "    'sasl.username': GKorir34,\n",
        "    'sasl.password': yuteiy89_yu7,\n",
        "    'group.id': 'network-traffic-group',\n",
        "    'auto.offset.reset': 'earliest'\n",
        "}\n",
        "\n",
        "# Create Kafka consumer\n",
        "consumer = Consumer(conf)\n",
        "consumer.subscribe([kafka_topic])\n",
        "\n",
        "def process_message(message):\n",
        "    nonlocal processed_data, event_times\n",
        "    if message is None:\n",
        "        return\n",
        "    if message.error():\n",
        "        if message.error().code() == KafkaError._PARTITION_EOF:\n",
        "            return\n",
        "        else:\n",
        "            st.error(f\"Error: {message.error()}\")\n",
        "            return\n",
        "\n",
        "    value = message.value().decode('utf-8')\n",
        "    data = value.split(',')\n",
        "    source_ip = data[0]\n",
        "    destination_ip = data[1]\n",
        "    bytes_sent = int(data[2])\n",
        "    event_time = datetime.now()\n",
        "\n",
        "    processed_data.append(bytes_sent)\n",
        "    event_times.append(event_time)\n",
        "    ax.clear()\n",
        "    ax.plot(event_times, processed_data)\n",
        "    ax.set_xlabel(\"Event Time\")\n",
        "    ax.set_ylabel(\"Processed Data\")\n",
        "    st.pyplot(fig)\n",
        "\n",
        "def read_from_kafka():\n",
        "    while True:\n",
        "        message = consumer.poll(1.0)\n",
        "\n",
        "        if message is not None:\n",
        "            process_message(message)\n",
        "\n",
        "try:\n",
        "    read_from_kafka()\n",
        "except Exception as e:\n",
        "    st.error(f\"An error occurred: {str(e)}\")\n",
        "finally:\n",
        "    consumer.close()\n",
        "    spark.stop()\n"
      ],
      "metadata": {
        "id": "edl3BfVsgTyq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}